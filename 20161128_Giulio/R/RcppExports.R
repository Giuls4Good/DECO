# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Square root of a symmetric matrix
#'
#' @param M a symmetric matrix
#' @return A matrix C which is the square root matrix of M (i.e. C*C=M)
#' @author Samuel Davenport, Jack Carter, Giulio Morina, Jeremias Knoblauch
#' @details To compute the square root of the matrix, \code{sqrtmat_sympd} function of Armadillo library is used.
#' @note This function is about 100 times faster than R function \code{sqrtm} (contained in expm package).
#'
#' Note that no check is done to test if the matrix M is actually symmetric.
#' @examples
#' require(expm)
#' require(rbenchmark)
#' A <- matrix(rnorm(10000,mean=10,sd=5),nrow=100)
#' A_symm <- A%*%t(A)
#' benchmark(sqrtm(A_symm),squareRootSymmetric(A_symm), order='relative')
#'
#' @export
squareRootSymmetric <- function(M) {
    .Call('RDeco_squareRootSymmetric', PACKAGE = 'RDeco', M)
}

#' Standardize a vector
#'
#' @param V a vector
#' @return Returns a vector with mean equal to 0.
#' @author Samuel Davenport, Jack Carter, Giulio Morina, Jeremias Knoblauch
#' @details To compute the standardized vector, each of its entries is subtracted with the vector's mean.
#' @note In general, this function does not return a vector with variance equal to 1.
#'
#' This function is about 2x slower than directly computing \code{v-mean(v)}, but it is faster than doing
#' \code{scale(v,scale=FALSE)}
#' @examples
#' require(rbenchmark)
#' v <- 1:5000000
#' benchmark({v-mean(v)},standardizeVector(v),scale(v,scale=FALSE), order='relative')
#'
#' @export
standardizeVector <- function(V) {
    .Call('RDeco_standardizeVector', PACKAGE = 'RDeco', V)
}

#' Standardize a matrix so that its mean is equal to 0
#'
#' @param M a matrix
#' @return A matrix with mean 0
#' @details The matrix is standardized by subtracting each column with the mean of that column.
#' @note In general, this function does not return a matrix with variance equal to 1.
#'
#' This function is about 2.5x times faster than doing \code{scale(M,scale=FALSE)}.
#' @examples
#' require(rbenchmark)
#' M <- matrix(rnorm(1000*5000,10,5), nrow=1000)
#' benchmark(scale(M,scale=FALSE),standardizeMatrix(M), order='relative')
#'
#' @export
standardizeMatrix <- function(M) {
    .Call('RDeco_standardizeMatrix', PACKAGE = 'RDeco', M)
}

#' Transpose of a matrix
#'
#' @param M a matrix
#' @return Its transpose
#' @details To compute the transpose matrix, Armadillo library is used.
#' @note This function is about 4 times slower than R function \code{t}.
#' @examples
#' require(rbenchmark)
#' M <- matrix(rnorm(1000*5000,10,5), nrow=1000)
#' benchmark(t(M),tMatrix(M),order='relative')
#'
#' @export
tMatrix <- function(M) {
    .Call('RDeco_tMatrix', PACKAGE = 'RDeco', M)
}

#' Multiply two matrices
#'
#' @param A a matrix
#' @param B a matrix whose size is compatible with the size of A
#' @return The matrix product A*B.
#' @details To compute such product, Armadillo library is used.
#' @note This function takes about the same speed as R matrix multiplication.
#' @examples
#' require(rbenchmark)
#' A <- matrix(rnorm(1000*500,10,5), nrow=1000, ncol=500)
#' B <- matrix(rnorm(1000*500,10,5), nrow=500, ncol=1000)
#' benchmark(A%*%B,mulMatrices(A,B),order='relative')
#'
#' @export
mulMatrices <- function(A, B) {
    .Call('RDeco_mulMatrices', PACKAGE = 'RDeco', A, B)
}

#' Inverse of a matrix
#'
#' @param M a symmetric quadratic matrix
#' @return The inverse of the matrix
#' @details To compute the square root of the matrix, \code{inv_sympd} function of Armadillo library is used.
#' @note This function is about 2.5x times faster than R function \code{solve}.
#'
#' Note that no check is done to test if the matrix M is actually symmetric.
#' @examples
#' require(rbenchmark)
#' M <- matrix(rnorm(1000^2,10,5), nrow=1000)
#' M_symm <- M%*%t(M)
#' benchmark(solve(M_symm),invSymmMatrix(M_symm),order='relative')
#'
#' @export
invSymmMatrix <- function(M) {
    .Call('RDeco_invSymmMatrix', PACKAGE = 'RDeco', M)
}

#' TITLE OF THIS FUNCTION
#'
#' @param beta gives the arma::vec vector of beta-estimates at the current iteration of Coordinate descent
#' @param X gives the arma::mat matrix of regressors in this partition of all regressors (there are m partitions
#'        in total, and we run coordinate descent on each partition)
#' @param Y gives the arma::vec vector of observations that we project X onto
#' @param lambda gives the double giving our lambda coefficient
#' @param n gives an integer corresponding to the number of rows (observations) of X (Y)
#' @param p gives the integer corresponding to the number of columns/regressors contained in X
#' @export
#' @return an updated version of beta
#'         NEEDS CHANGING:
#'         - we should compute Xi'Xi once for each i, and then supply it as another parameter?
#'         - we should be able to just return a pointer to a (changed) beta (more efficient than local copies!)
update_naive <- function(beta, X, Y, lambda, n, p) {
    .Call('RDeco_update_naive', PACKAGE = 'RDeco', beta, X, Y, lambda, n, p)
}

#' TITLE OF THIS FUNCTION
#'
#' @param X gives the arma::mat matrix of regressors in this partition of all regressors (there are m partitions
#'        in total, and we run coordinate descent on each partition)
#' @param Y gives the arma::vec vector of observations that we project X onto
#' @param lambda gives the double giving our lambda coefficient
#' @param precision gives the convergence criterion (how close two subsequent iterations should be before termination)
#' @param max_iter gives the maximum number of iterations in the inner update loop of coordinate descent
#' @export
#' @return an updated version of beta
#'         NEEDS CHANGING:
#'         - we should compute Xi'Xi once for each i, and then supply it as another parameter?
#'         - we should be able to just return a pointer to a (changed) beta (more efficient than local copies!)
coordinateDescent_naive <- function(X, Y, lambda, precision, max_iter) {
    .Call('RDeco_coordinateDescent_naive', PACKAGE = 'RDeco', X, Y, lambda, precision, max_iter)
}

#' DECO Parallelized Algorithm (Pure C)
#'
#' This function is deprecated. Use \code{DECO_LASSO_C_PARALLEL} function.
#'
#' @details This function is equivalent to \code{DECO_LASSO_C_PARALLEL} function when fixing \code{m=1, ncores=1}.
#'
#' @export
DECO_LASSO_C <- function(Y, X, p, n, lambda, r, ncores = 1L, intercept = TRUE) {
    .Call('RDeco_DECO_LASSO_C', PACKAGE = 'RDeco', Y, X, p, n, lambda, r, ncores, intercept)
}

#' DECO Parallelized Algorithm (Pure C++)
#'
#' The algorithm is based on the description in "DECOrrelated feature space partitioning
#' for distributed sparse regression" in Wang, Dunson, and Leng (2016) if lambda is fixed and
#' LASSO is used as the penalized regression scheme.
#'
#' @param Y gives the nx1 vector of observations we wish to approximate with a linear model of type Y = Xb + e
#' @param X gives the nxp matrix of regressors, each column corresponding to a different regressor
#' @param p is the column dimension of X [equivalently, p is the number of regressor variables].
#' If not given, it is computed as the number of columns of X.
#' @param n is the row dimension of X (and Y) [equivalently, n is the number of observations/individuals]
#'  If not given, it is computed as the number of rows of X.
#' @param m is the number of groups/blocks you wish to split X into, denoted X(i) for 1 <= i <= m
#' @param lambda gives the (fixed) penalty magnitude in the LASSO fit of the algorithm
#' @param ncores determines the number of cores used on each machine to parallelize computation
#' @param r_1 is a tweaking parameter for making the inverse more robust (as we take inverse of XX + r_1*I)
#' @param r_2 is a tweaking parameter for making the inverse more robust (as we take inverse of X_MX_M + r_2*I)
#' @param intercept determines whether to include an intercept in the model or not
#' @param refinement determines whether to include the refinement step (Stage 3 of the algorithm)
#' @param parallel_glmnet determines whether a parallel version of the Lasso coefficients should be used.
#' This parameter is ignored when \code{glmnet} is set to \code{FALSE} (see details).
#' @param glmnet determines whether glmnet function form glmnet R package should be used to compute the Lasso coefficients.
#' See details for further information. If set to \code{FALSE}, C++ implementation of coordinate descent algorithm is used.
#' @param precision determines the precision used in the coordinate descent algorithm. It is ignored when
#' \code{glmnet} is set to \code{TRUE}.
#' @param max_iter determines the maximum number of iterations used in the coordinate descent algorithm.
#' It is ignored when \code{glmnet} is set to \code{TRUE}.
#' @details This function is a C++ implementation of \code{DECO_LASSO_R} and \code{DECO_LASSO_MIX} functions.
#' Due to the fact that it is entirely written in C++ is generally way faster than its counterparts.
#'
#' Two functions can be used to compute Lasso coefficients: glmnet R function (\code{glmnet = TRUE})
#' and coordinate descent algorithm (\code{glmnet = FALSE}). glmnet R function is generally faster, but more memory is
#' required to pass the input argumentd from C++ to R and back. When \code{parallel_glmnet = TRUE} an R parallelized
#' version of glmnet is used. Note however that for small datasets this could lead to slower run times, due to the
#' communication between C++ and R.
#'
#' Descent coordinate algorithm is always run in a parallel way (using \code{ncores} threads).
#'
#' @return An estimate of the coefficients b.
#' @author Samuel Davenport, Jack Carter, Giulio Morina, Jeremias Knoblauch
#' @export
DECO_LASSO_C_PARALLEL <- function(Y, X, p, n, m, lambda, r_1, r_2 = 0.01, ncores = 1L, intercept = TRUE, refinement = TRUE, glmnet = TRUE, parallel_glmnet = FALSE, precision = 0.0000001, max_iter = 100000L) {
    .Call('RDeco_DECO_LASSO_C_PARALLEL', PACKAGE = 'RDeco', Y, X, p, n, m, lambda, r_1, r_2, ncores, intercept, refinement, glmnet, parallel_glmnet, precision, max_iter)
}

